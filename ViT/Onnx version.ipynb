{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62b7006-437c-4110-af7c-5eed62444a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hari\\AppData\\Local\\Temp\\ipykernel_17228\\3971327776.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_weightsv2.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model architecture\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model_checkpoint = \"motheecreator/vit-Facial-Expression-Recognition\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(\"model_weightsv2.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272c33ab-ea30-4bb2-a78e-e36755175859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ed0ccd-2238-4b31-9c7c-440697066afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.onnx.export(\n",
    "    model,                     # your model\n",
    "    dummy_input,               # dummy input\n",
    "    \"model_v2.onnx\",           # where to save the ONNX file\n",
    "    export_params=True,        # store trained weights\n",
    "    opset_version=14,          # ONNX version\n",
    "    do_constant_folding=True,  # fold constants for optimization\n",
    "    input_names=['input'],     # input name\n",
    "    output_names=['output'],   # output name\n",
    "    dynamic_axes={             # allow variable batch size\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10a66dd-1e9d-4385-a7b3-f1bcdfee20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX output: [array([[-0.35294837, -1.1266174 , -1.8066499 ,  1.0692861 ,  1.363068  ,\n",
      "         0.20152882, -0.57816905]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = ort.InferenceSession(\"model_v2.onnx\")\n",
    "\n",
    "# Convert the dummy_input to numpy\n",
    "onnx_input = dummy_input.numpy()\n",
    "\n",
    "# Run inference\n",
    "outputs = ort_session.run(None, {'input': onnx_input})\n",
    "print(\"ONNX output:\", outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24009ea9-4e77-4bd3-8ad1-8be3287d9ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n",
      "\n",
      "0: 480x640 1 face, 71.4ms\n",
      "Speed: 2.4ms preprocess, 71.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 74.6ms\n",
      "Speed: 2.5ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 81.2ms\n",
      "Speed: 2.6ms preprocess, 81.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 76.5ms\n",
      "Speed: 1.9ms preprocess, 76.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 72.6ms\n",
      "Speed: 1.9ms preprocess, 72.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 67.7ms\n",
      "Speed: 1.7ms preprocess, 67.7ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 65.3ms\n",
      "Speed: 1.8ms preprocess, 65.3ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 64.4ms\n",
      "Speed: 1.7ms preprocess, 64.4ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 70.2ms\n",
      "Speed: 2.1ms preprocess, 70.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 faces, 82.7ms\n",
      "Speed: 2.4ms preprocess, 82.7ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class EmotionDetectionSystem:\n",
    "    def __init__(self, yolo_model_path, vit_onnx_path, emotion_labels):\n",
    "        \"\"\"\n",
    "        Initialize the system with YOLO and ViT ONNX models\n",
    "        \"\"\"\n",
    "        # Load YOLOv10 model\n",
    "        self.yolo_model = YOLO(yolo_model_path)\n",
    "        \n",
    "        # Load ONNX model\n",
    "        self.ort_session = ort.InferenceSession(vit_onnx_path, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "        self.input_name = self.ort_session.get_inputs()[0].name\n",
    "        \n",
    "        # Emotion labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "\n",
    "        # Define image transformations for ONNX input (same as ViT expected format)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.frame_count = 0\n",
    "        self.start_time = time.time()\n",
    "        self.fps = 0\n",
    "    \n",
    "    def preprocess_face(self, face_img):\n",
    "        \"\"\"\n",
    "        Preprocess face image for ONNX model\n",
    "        \"\"\"\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "        img_tensor = self.transform(pil_img).unsqueeze(0).numpy().astype(np.float32)\n",
    "        return img_tensor\n",
    "    \n",
    "    def predict_emotion(self, face_tensor):\n",
    "        \"\"\"\n",
    "        Predict emotion using ONNX ViT model\n",
    "        \"\"\"\n",
    "        outputs = self.ort_session.run(None, {self.input_name: face_tensor})\n",
    "        logits = outputs[0]\n",
    "        probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "        pred_idx = np.argmax(probabilities)\n",
    "        emotion = self.emotion_labels[pred_idx]\n",
    "        confidence = probabilities[0][pred_idx]\n",
    "        return emotion, confidence\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        results = self.yolo_model(frame)\n",
    "        detections = results[0].boxes\n",
    "        \n",
    "        self.frame_count += 1\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > 1.0:\n",
    "            self.fps = self.frame_count / elapsed_time\n",
    "            self.frame_count = 0\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        cv2.putText(frame, f\"FPS: {self.fps:.1f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "            if face_img.size == 0 or face_img.shape[0] < 20 or face_img.shape[1] < 20:\n",
    "                continue\n",
    "            try:\n",
    "                face_tensor = self.preprocess_face(face_img)\n",
    "                emotion, confidence = self.predict_emotion(face_tensor)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "                label = f\"{emotion} ({confidence*100:.1f}%)\"\n",
    "                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                text_x = x1 + (x2 - x1 - w) // 2\n",
    "                cv2.putText(frame, label, (text_x, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing face: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def run_webcam(self, camera_id=0):\n",
    "        cap = cv2.VideoCapture(camera_id)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            return\n",
    "        print(\"Press 'q' to quit\")\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            processed_frame = self.process_frame(frame)\n",
    "            cv2.imshow('Emotion Detection', processed_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "if __name__ == \"__main__\":\n",
    "    yolo_model_path = r\"C:\\Users\\hari\\runs\\detect\\train\\weights\\best.pt\"\n",
    "    vit_onnx_path = \"model_v2.onnx\"  # <-- your ONNX ViT model here\n",
    "\n",
    "    emotion_labels = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
    "\n",
    "    emotion_system = EmotionDetectionSystem(\n",
    "        yolo_model_path=yolo_model_path,\n",
    "        vit_onnx_path=vit_onnx_path,\n",
    "        emotion_labels=emotion_labels\n",
    "    )\n",
    "\n",
    "    emotion_system.run_webcam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaba3bb-0a6b-40a5-9dd6-bcc4b9ff4e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51772562-e533-4b37-854a-811a5500bbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
